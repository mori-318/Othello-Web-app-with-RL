{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10bbac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_DIR: /Users/mori/dev/Othello-Web-app-with-RL/rl_agent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Optional\n",
    "from statistics import mean\n",
    "\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "BASE_DIR = str(pathlib.Path(os.getcwd()).parent)\n",
    "sys.path.append(str(BASE_DIR))\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516bd3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = os.path.join(BASE_DIR, \"models\")\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "weights_path = os.path.join(model_save_dir, \"v2.json\")\n",
    "\n",
    "db_dir = os.path.join(BASE_DIR, \"dbs\")\n",
    "os.makedirs(db_dir, exist_ok=True)\n",
    "optuna_db_path = os.path.join(db_dir, \"v2.db\")\n",
    "\n",
    "EMPTY, BLACK, WHITE = 0, 1, -1\n",
    "DIRECTIONS = [(-1,-1),(-1,0),(-1,1),(0,-1),(0,1),(1,-1),(1,0),(1,1)]\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2426b5f",
   "metadata": {},
   "source": [
    "## Othello環境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b408c1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Othello:\n",
    "    def __init__(self):\n",
    "        self.board = [[EMPTY for _ in range(8)] for _ in range(8)]\n",
    "        self.board[3][3] = self.board[4][4] = WHITE\n",
    "        self.board[3][4] = self.board[4][3] = BLACK\n",
    "        self.player = BLACK\n",
    "\n",
    "    def clone(self):\n",
    "        g = Othello()\n",
    "        g.board = [row[:] for row in self.board]\n",
    "        g.player = self.player\n",
    "        return g\n",
    "\n",
    "    def inside(self, r, c): return 0 <= r < 8 and 0 <= c < 8\n",
    "\n",
    "    def legal_moves(self, player=None) -> List[Tuple[int, int]]:\n",
    "        if player is None: player = self.player\n",
    "        moves = []\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if self.board[r][c] != EMPTY: continue\n",
    "                if self._would_flip(r, c, player):\n",
    "                    moves.append((r, c))\n",
    "        return moves\n",
    "\n",
    "    def _would_flip(self, r, c, player) -> bool:\n",
    "        if self.board[r][c] != EMPTY: return False\n",
    "        for dr, dc in DIRECTIONS:\n",
    "            rr, cc = r + dr, c + dc\n",
    "            seen_opp = False\n",
    "            while self.inside(rr, cc) and self.board[rr][cc] == opponent(player):\n",
    "                seen_opp = True\n",
    "                rr += dr; cc += dc\n",
    "            if seen_opp and self.inside(rr, cc) and self.board[rr][cc] == player:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def play(self, r, c, player=None):\n",
    "        if player is None: player = self.player\n",
    "        assert self.board[r][c] == EMPTY\n",
    "        flipped = []\n",
    "        for dr, dc in DIRECTIONS:\n",
    "            line = []\n",
    "            rr, cc = r + dr, c + dc\n",
    "            while self.inside(rr,cc) and self.board[rr][cc] == opponent(player):\n",
    "                line.append((rr,cc))\n",
    "                rr += dr; cc += dc\n",
    "            if line and self.inside(rr,cc) and self.board[rr][cc] == player:\n",
    "                flipped.extend(line)\n",
    "        if not flipped: raise ValueError(\"Illegal move\")\n",
    "        self.board[r][c] = player\n",
    "        for rr,cc in flipped: self.board[rr][cc] = player\n",
    "        self.player = opponent(player)\n",
    "        if not self.legal_moves(self.player):\n",
    "            self.player = opponent(self.player)\n",
    "\n",
    "    def terminal(self) -> bool:\n",
    "        if self.legal_moves(BLACK): return False\n",
    "        if self.legal_moves(WHITE): return False\n",
    "        return True\n",
    "\n",
    "    def score(self) -> int:\n",
    "        s = 0\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                s += self.board[r][c]\n",
    "        return s\n",
    "\n",
    "    def winner(self) -> int:\n",
    "        s = self.score()\n",
    "        return BLACK if s > 0 else WHITE if s < 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb2f0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_systematic_2tuples() -> List[Tuple[Tuple[int,int], ...]]:\n",
    "    \"\"\"8x8上の隣接ペアを系統的に列挙（横・縦・斜め）\"\"\"\n",
    "    tuples = []\n",
    "    # 横\n",
    "    for r in range(8):\n",
    "        for c in range(7):\n",
    "            tuples.append(((r,c),(r,c+1)))\n",
    "    # 縦\n",
    "    for r in range(7):\n",
    "        for c in range(8):\n",
    "            tuples.append(((r,c),(r+1,c)))\n",
    "    # 斜め（↘）\n",
    "    for r in range(7):\n",
    "        for c in range(7):\n",
    "            tuples.append(((r,c),(r+1,c+1)))\n",
    "    # 斜め（↙）\n",
    "    for r in range(7):\n",
    "        for c in range(1,8):\n",
    "            tuples.append(((r,c),(r+1,c-1)))\n",
    "    return tuples  # 合計: 56 + 56 + 49 + 49 = 210 本、各テーブルサイズ 3^2=9\n",
    "\n",
    "class NTupleValue:\n",
    "    \"\"\"\n",
    "    n-tuple ネットワーク（3進エンコード）\n",
    "    - 盤面は「手番プレイヤー視点」で符号を反転せず評価\n",
    "    - 各タプルは {自=2, 空=1, 相手=0} の3値をbase-3でインデックス化\n",
    "    \"\"\"\n",
    "    __slots__ = (\"tuples\", \"tables\", \"base_pow\", \"n_trits\", \"rng\")\n",
    "\n",
    "    def __init__(self, tuples=None, seed=0):\n",
    "        self.tuples = tuples if tuples is not None else generate_systematic_2tuples()\n",
    "        self.n_trits = 3  # 自/空/相手\n",
    "        # 各タプルの長さ（ここでは全て2）\n",
    "        self.base_pow = [3**i for i in range(8)]  # 最大長8まで対応\n",
    "        self.tables = [np.zeros(self.n_trits**len(t), dtype=np.float64) for t in self.tuples]\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cell_trit(v: int, player: int) -> int:\n",
    "        # 自=2, 空=1, 相手=0\n",
    "        if v == player: return 2\n",
    "        if v == EMPTY: return 1\n",
    "        return 0\n",
    "\n",
    "    def indices_for(self, game: Othello, player: int) -> List[int]:\n",
    "        b = game.board\n",
    "        idxs = []\n",
    "        for t in self.tuples:\n",
    "            s = 0\n",
    "            for i, (r,c) in enumerate(t):\n",
    "                s += self._cell_trit(b[r][c], player) * self.base_pow[i]\n",
    "            idxs.append(s)\n",
    "        return idxs\n",
    "\n",
    "    def value_from_indices(self, idxs: List[int]) -> float:\n",
    "        v = 0.0\n",
    "        for tbl, idx in zip(self.tables, idxs):\n",
    "            v += tbl[idx]\n",
    "        return v\n",
    "\n",
    "    def value(self, game: Othello, player: int) -> float:\n",
    "        return self.value_from_indices(self.indices_for(game, player))\n",
    "\n",
    "    def update(self, idxs: List[int], target: float, alpha: float):\n",
    "        v = self.value_from_indices(idxs)\n",
    "        delta = target - v\n",
    "        a = alpha * delta / len(self.tables)  # 均等配分で学習安定化\n",
    "        for tbl, idx in zip(self.tables, idxs):\n",
    "            tbl[idx] += a\n",
    "        return delta\n",
    "\n",
    "    # 便利：保存/読み込み\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"tuples\": self.tuples,\n",
    "            \"tables\": [tbl.tolist() for tbl in self.tables]\n",
    "        }\n",
    "    @classmethod\n",
    "    def from_dict(cls, d):\n",
    "        obj = cls(tuples=[tuple(map(tuple, t)) for t in d[\"tuples\"]])\n",
    "        obj.tables = [np.array(t, dtype=np.float64) for t in d[\"tables\"]]\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760507f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 方策（ε-greedy）と対局\n",
    "# ------------------------\n",
    "def choose_move(game: Othello, player: int, V: NTupleValue, eps=0.1):\n",
    "    moves = game.legal_moves(player)\n",
    "    if not moves: return None\n",
    "    if random.random() < eps:\n",
    "        return random.choice(moves)\n",
    "    best_v = -1e18\n",
    "    best_m = moves[0]\n",
    "    for m in moves:\n",
    "        g2 = game.clone(); g2.play(m[0], m[1], player)\n",
    "        v = V.value(g2, player)\n",
    "        if v > best_v:\n",
    "            best_v = v; best_m = m\n",
    "    return best_m\n",
    "\n",
    "# ------------------------\n",
    "# 学習ループ・評価\n",
    "# ------------------------\n",
    "def train(num_games=5000, alpha=0.01, my_eps=0.2, seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    V = NTupleValue(seed=seed)\n",
    "    results = {BLACK:0, WHITE:0, 0:0}\n",
    "    history = []\n",
    "    for i in range(1, num_games+1):\n",
    "        # 学習時：自分は ε-貪欲、相手はヒューリスティック例\n",
    "        w = play_one_game_for_train(\n",
    "            V, alpha=alpha, learn=True,\n",
    "            my_policy=\"eps_greedy\", my_eps=my_eps,\n",
    "            opp_policy=\"heuristic\", opp_V=None, opp_eps=0.05\n",
    "        )\n",
    "        results[w] += 1\n",
    "        if i % 100 == 0:\n",
    "            total = i\n",
    "            history.append({\n",
    "                \"game\": i,\n",
    "                \"black\": results[BLACK]/total*100,\n",
    "                \"white\": results[WHITE]/total*100,\n",
    "                \"draw\": results[0]/total*100,\n",
    "            })\n",
    "            print(f\"[{i}] B:{results[BLACK]/total*100:.1f}% W:{results[WHITE]/total*100:.1f}% D:{results[0]/total*100:.1f}%\")\n",
    "    return V, history\n",
    "\n",
    "def plot_train_history(train_history):\n",
    "    xs = [h['game'] for h in train_history]\n",
    "    b = [h[\"black\"] for h in train_history]\n",
    "    w = [h[\"white\"] for h in train_history]\n",
    "    d = [h[\"draw\"] for h in train_history]\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(xs, b, label=\"Black Win %\")\n",
    "    plt.plot(xs, w, label=\"White Win %\")\n",
    "    plt.plot(xs, d, label=\"Draw %\")\n",
    "    plt.xlabel(\"Games\"); plt.ylabel(\"Win Rate %\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "def play_match(V: NTupleValue, eps=0.0, games=50, seed=SEED):\n",
    "    random.seed(seed)\n",
    "    res = {BLACK:0, WHITE:0, 0:0}\n",
    "    for _ in range(games):\n",
    "        w = play_one_game(V, alpha=0.0, eps=eps, learn=False)\n",
    "        res[w] += 1\n",
    "    print(f\"vs greedy self  ({games} games): B={res[BLACK]} W={res[WHITE]} D={res[0]}\")\n",
    "    return res\n",
    "\n",
    "def _eval_afterstate(g_after: Othello, player: int, V):\n",
    "    \"\"\"\n",
    "    afterstate g_after を V で評価してスカラーを返す。\n",
    "    V が NTupleValue でも LinearValue でも動く。\n",
    "    \"\"\"\n",
    "    # NTupleValue: value(game, player)\n",
    "    if hasattr(V, \"indices_for\") and hasattr(V, \"value\"):\n",
    "        return V.value(g_after, player)\n",
    "    # LinearValue: value(features(game, player))\n",
    "    elif hasattr(V, \"w\"):\n",
    "        return V.value(features(g_after, player))\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported value function type for evaluation\")\n",
    "\n",
    "def select_move(\n",
    "    game: Othello,\n",
    "    player: int,\n",
    "    V,\n",
    "    policy: str = \"eps_greedy\",   # \"eps_greedy\" | \"greedy\" | \"random\" | \"heuristic\"\n",
    "    eps: float = 0.1,\n",
    "    tol: float = 1e-12,\n",
    "):\n",
    "    \"\"\"\n",
    "    学習/評価兼用の次手選択。V は NTupleValue でも LinearValue でもOK。\n",
    "    \"\"\"\n",
    "    moves = game.legal_moves(player)\n",
    "    if not moves:\n",
    "        return None\n",
    "\n",
    "    # 完全ランダム\n",
    "    if policy == \"random\":\n",
    "        return random.choice(moves)\n",
    "\n",
    "    # ヒューリスティック（角→X回避→辺優先、同値はVで貪欲。Vが無ければランダム）\n",
    "    if policy == \"heuristic\":\n",
    "        corners = {(0,0),(0,7),(7,0),(7,7)}\n",
    "        xcells = {(0,1),(1,0),(1,1),(0,6),(1,6),(1,7),(6,0),(6,1),(7,1),(6,6),(6,7),(7,6)}\n",
    "        edges = set([(0,c) for c in range(2,6)] + [(7,c) for c in range(2,6)] +\n",
    "                    [(r,0) for r in range(2,6)] + [(r,7) for r in range(2,6)])\n",
    "\n",
    "        # 1) 角があれば角。同値は V で選ぶ（なければランダム）\n",
    "        corner_moves = [m for m in moves if m in corners]\n",
    "        if corner_moves:\n",
    "            if V is None:\n",
    "                return random.choice(corner_moves)\n",
    "            best_v, cands = -1e18, []\n",
    "            for m in corner_moves:\n",
    "                g2 = game.clone(); g2.play(m[0], m[1], player)\n",
    "                v = _eval_afterstate(g2, player, V)\n",
    "                if v > best_v + tol: best_v, cands = v, [m]\n",
    "                elif abs(v - best_v) <= tol: cands.append(m)\n",
    "            return random.choice(cands)\n",
    "\n",
    "        # 2) X回避 → 3) 辺優先\n",
    "        pool = [m for m in moves if m not in xcells] or moves\n",
    "        edge_pool = [m for m in pool if m in edges] or pool\n",
    "\n",
    "        # 少しだけランダム性（epsで制御）\n",
    "        if random.random() < eps:\n",
    "            return random.choice(edge_pool)\n",
    "\n",
    "        # 残りは V で貪欲（Vが無ければランダム）\n",
    "        if V is None:\n",
    "            return random.choice(edge_pool)\n",
    "        best_v, cands = -1e18, []\n",
    "        for m in edge_pool:\n",
    "            g2 = game.clone(); g2.play(m[0], m[1], player)\n",
    "            v = _eval_afterstate(g2, player, V)\n",
    "            if v > best_v + tol: best_v, cands = v, [m]\n",
    "            elif abs(v - best_v) <= tol: cands.append(m)\n",
    "        return random.choice(cands)\n",
    "\n",
    "    # ここからは V を使うモード（greedy / eps_greedy）\n",
    "    assert V is not None, \"greedy/eps_greedy policy requires V\"\n",
    "\n",
    "    # ε-貪欲\n",
    "    if policy == \"eps_greedy\":\n",
    "        if random.random() < eps:\n",
    "            return random.choice(moves)\n",
    "        # fallthroughして貪欲へ\n",
    "\n",
    "    # 貪欲\n",
    "    best_v = -1e18\n",
    "    best_m = moves[0]\n",
    "    for m in moves:\n",
    "        g2 = game.clone(); g2.play(m[0], m[1], player)\n",
    "        v = _eval_afterstate(g2, player, V)\n",
    "        if v > best_v:\n",
    "            best_v, best_m = v, m\n",
    "    return best_m\n",
    "\n",
    "def play_one_game_for_train(\n",
    "    V: NTupleValue,\n",
    "    alpha=0.01,\n",
    "    learn=True,\n",
    "    my_policy=\"eps_greedy\",   # ← 追加: 自分の選択モード\n",
    "    my_eps=0.2,               # ← 追加: 自分のε\n",
    "    opp_policy=\"heuristic\",   # ← 追加: 相手の選択モード\n",
    "    opp_V=None,               # ← 追加: 相手がモデル貪欲/ε-貪欲ならそのV（Linear/NTuple）\n",
    "    opp_eps=0.05              # ← 追加: 相手のε\n",
    "):\n",
    "    g = Othello()\n",
    "    histories = []  # (idxs_after, player)\n",
    "\n",
    "    while not g.terminal():\n",
    "        p = g.player\n",
    "        if p == BLACK:\n",
    "            m = select_move(g, p, V, policy=my_policy, eps=my_eps)\n",
    "        else:\n",
    "            m = select_move(g, p, opp_V if opp_policy in (\"greedy\",\"eps_greedy\") else V,\n",
    "                            policy=opp_policy, eps=opp_eps)\n",
    "        if m is None:\n",
    "            g.player = opponent(g.player)\n",
    "            continue\n",
    "        g2 = g.clone(); g2.play(m[0], m[1], p)\n",
    "        idxs = V.indices_for(g2, p)  # afterstate\n",
    "        histories.append((idxs, p))\n",
    "        g.play(m[0], m[1], p)\n",
    "\n",
    "    w = g.winner()\n",
    "    reward_black = 1.0 if w == BLACK else -1.0 if w == WHITE else 0.0\n",
    "\n",
    "    if learn:\n",
    "        for idxs, p in histories:\n",
    "            target = reward_black if p == BLACK else -reward_black\n",
    "            V.update(idxs, target, alpha)\n",
    "    return w\n",
    "\n",
    "def evaluate_agent(V_my, games_per_color=50, opp_type=\"random\", opp_V=None, opp_eps=0.05, seed=2025):\n",
    "    \"\"\"\n",
    "    色交代の平均勝率（0〜1）を返す。\n",
    "    BLACK/WHITE それぞれ games_per_color 対局し合計2*games_per_color。\n",
    "    Args:\n",
    "        V_my: 自分の価値関数\n",
    "        games_per_color: 各色の対局数\n",
    "        opp_type: 相手のタイプ\n",
    "        opp_V: 相手の価値関数\n",
    "        opp_eps: 相手のε\n",
    "        seed: seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    wins = draws = losses = 0\n",
    "    # 自分が黒\n",
    "    for _ in range(games_per_color):\n",
    "        r = play_one_game_fixed_opponent(V_my, V_opp, my_color=BLACK, opp_eps=opp_eps)\n",
    "        if r > 0: wins += 1\n",
    "        elif r < 0: losses += 1\n",
    "        else: draws += 1\n",
    "    # 自分が白\n",
    "    for _ in range(games_per_color):\n",
    "        r = play_one_game_fixed_opponent(V_my, V_opp, my_color=WHITE, opp_eps=opp_eps)\n",
    "        if r > 0: wins += 1\n",
    "        elif r < 0: losses += 1\n",
    "        else: draws += 1\n",
    "    total = 2 * games_per_color\n",
    "    winrate = (wins + 0.5 * draws) / total\n",
    "    return {\"wins\": wins, \"losses\": losses, \"draws\": draws, \"total\": total, \"winrate\": winrate}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9777e23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# 対人対局（実戦は eps=0.0）\n",
    "# ------------------------\n",
    "def print_board(game: Othello):\n",
    "    b = game.board\n",
    "    header = \"   \" + \" \".join([chr(ord('a')+c) for c in range(8)])\n",
    "    print(header)\n",
    "    for r in range(8):\n",
    "        line = f\"{r+1:2d} \"\n",
    "        for c in range(8):\n",
    "            if b[r][c] == BLACK: ch = \"○\"\n",
    "            elif b[r][c] == WHITE: ch = \"●\"\n",
    "            else: ch = \".\"\n",
    "            line += ch + \" \"\n",
    "        print(line)\n",
    "    turn = \"Black(○)\" if game.player == BLACK else \"White(●)\"\n",
    "    print(f\"Turn: {turn}\")\n",
    "\n",
    "def parse_move(s: str):\n",
    "    s = s.strip().lower()\n",
    "    if s in (\"pass\", \"p\"): return None\n",
    "    if len(s) != 2: return \"ERR\"\n",
    "    col = ord(s[0]) - ord('a')\n",
    "    row = ord(s[1]) - ord('1')\n",
    "    if 0 <= row < 8 and 0 <= col < 8: return (row, col)\n",
    "    return \"ERR\"\n",
    "\n",
    "def human_vs_agent(V: NTupleValue, human_color=BLACK, seed=123):\n",
    "    random.seed(seed)\n",
    "    g = Othello()\n",
    "    while not g.terminal():\n",
    "        print_board(g)\n",
    "        p = g.player\n",
    "        if p == human_color:\n",
    "            moves = g.legal_moves(p)\n",
    "            if not moves:\n",
    "                print(\"No legal moves. Pass.\")\n",
    "                g.player = opponent(g.player)\n",
    "                continue\n",
    "            s = input(\"Your move (e.g., d3 / pass / quit): \").strip().lower()\n",
    "            if s in (\"quit\", \"q\"):\n",
    "                print(\"Quit.\"); return\n",
    "            mv = parse_move(s)\n",
    "            if mv == \"ERR\":\n",
    "                print(\"Format error. Try again.\"); continue\n",
    "            if mv is None:\n",
    "                g.player = opponent(g.player); continue\n",
    "            if mv not in moves:\n",
    "                print(\"Illegal. Try again.\"); continue\n",
    "            g.play(mv[0], mv[1], p)\n",
    "        else:\n",
    "            m = choose_move(g, p, V, eps=0.0)  # 実戦は探索0\n",
    "            if m is None:\n",
    "                print(\"(Agent) Pass.\")\n",
    "                g.player = opponent(g.player); continue\n",
    "            print(f\"(Agent) move: {chr(ord('a')+m[1])}{m[0]+1}\")\n",
    "            g.play(m[0], m[1], p)\n",
    "\n",
    "    print_board(g)\n",
    "    w = g.winner()\n",
    "    if w == 0: print(\"Draw!\")\n",
    "    elif w == BLACK: print(\"Black(○) wins!\")\n",
    "    else: print(\"White(●) wins!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3939319",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 00:21:59,640] Using an existing study with name 'othello_ntuple_eval' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] B:51.0% W:48.0% D:1.0%\n",
      "[200] B:52.0% W:47.5% D:0.5%\n",
      "[300] B:58.0% W:41.7% D:0.3%\n",
      "[400] B:61.0% W:38.8% D:0.2%\n",
      "[500] B:62.6% W:37.0% D:0.4%\n",
      "[600] B:64.3% W:35.3% D:0.3%\n",
      "[700] B:64.7% W:35.0% D:0.3%\n",
      "[800] B:65.5% W:33.9% D:0.6%\n",
      "[900] B:65.4% W:33.8% D:0.8%\n",
      "[1000] B:66.5% W:32.6% D:0.9%\n",
      "[1100] B:66.9% W:32.0% D:1.1%\n",
      "[1200] B:67.2% W:31.8% D:1.0%\n",
      "[1300] B:67.5% W:31.5% D:1.0%\n",
      "[1400] B:67.2% W:31.7% D:1.1%\n",
      "[1500] B:66.1% W:32.9% D:1.0%\n",
      "[1600] B:65.7% W:33.3% D:1.0%\n",
      "[1700] B:66.1% W:32.8% D:1.1%\n",
      "[1800] B:66.1% W:32.8% D:1.1%\n",
      "[1900] B:66.3% W:32.6% D:1.1%\n",
      "[2000] B:66.0% W:32.9% D:1.1%\n",
      "[2100] B:65.5% W:33.3% D:1.1%\n",
      "[2200] B:65.5% W:33.2% D:1.3%\n",
      "[2300] B:65.0% W:33.7% D:1.3%\n",
      "[2400] B:63.7% W:35.0% D:1.2%\n",
      "[2500] B:62.7% W:36.0% D:1.3%\n",
      "[2600] B:61.6% W:37.0% D:1.3%\n",
      "[2700] B:60.3% W:38.3% D:1.3%\n",
      "[2800] B:58.8% W:39.9% D:1.3%\n",
      "[2900] B:57.6% W:41.2% D:1.2%\n",
      "[3000] B:56.4% W:42.3% D:1.3%\n",
      "[3100] B:55.2% W:43.5% D:1.4%\n",
      "[3200] B:54.3% W:44.3% D:1.4%\n",
      "[3300] B:53.4% W:45.2% D:1.4%\n",
      "[3400] B:52.6% W:46.0% D:1.4%\n",
      "[3500] B:52.1% W:46.5% D:1.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-08-11 00:25:33,251] Trial 1 failed with parameters: {'alpha': 0.0010253509690168491, 'eps': 0.17254716573280354, 'num_games': 5000} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mori/dev/Othello-Web-app-with-RL/rl_agent/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/mx/r5dnnbr55_b4_pyzp04zlqtm0000gn/T/ipykernel_16554/3678377591.py\", line 27, in objective\n",
      "    V = train_with_report(num_games, alpha, eps, s, trial=trial, report_every=200)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/mx/r5dnnbr55_b4_pyzp04zlqtm0000gn/T/ipykernel_16554/3678377591.py\", line 6, in train_with_report\n",
      "    V, hist = train(num_games=num_games, alpha=alpha, eps=eps, seed=seed)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/mx/r5dnnbr55_b4_pyzp04zlqtm0000gn/T/ipykernel_16554/1092643393.py\", line 10, in train\n",
      "    w = play_one_game(V, alpha=alpha, eps=eps, learn=True)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/mx/r5dnnbr55_b4_pyzp04zlqtm0000gn/T/ipykernel_16554/51706225.py\", line 80, in play_one_game\n",
      "    m = choose_move(g, p, V, eps=eps)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/mx/r5dnnbr55_b4_pyzp04zlqtm0000gn/T/ipykernel_16554/51706225.py\", line 13, in choose_move\n",
      "    v = V.value(g2, player)\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/mx/r5dnnbr55_b4_pyzp04zlqtm0000gn/T/ipykernel_16554/1496810690.py\", line 62, in value\n",
      "    return self.value_from_indices(self.indices_for(game, player))\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/mx/r5dnnbr55_b4_pyzp04zlqtm0000gn/T/ipykernel_16554/1496810690.py\", line 51, in indices_for\n",
      "    s += self._cell_trit(b[r][c], player) * self.base_pow[i]\n",
      "    ^\n",
      "KeyboardInterrupt\n",
      "[W 2025-08-11 00:25:33,252] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     35\u001b[39m pruner  = MedianPruner(n_startup_trials=\u001b[32m5\u001b[39m, n_warmup_steps=\u001b[32m5\u001b[39m, interval_steps=\u001b[32m1\u001b[39m)\n\u001b[32m     37\u001b[39m study = optuna.create_study(\n\u001b[32m     38\u001b[39m     study_name=\u001b[33m\"\u001b[39m\u001b[33mothello_ntuple_eval\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     storage=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msqlite:///\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptuna_db_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m bt = study.best_trial\n\u001b[32m     48\u001b[39m params = bt.params\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Othello-Web-app-with-RL/rl_agent/.venv/lib/python3.12/site-packages/optuna/study/study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Othello-Web-app-with-RL/rl_agent/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Othello-Web-app-with-RL/rl_agent/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Othello-Web-app-with-RL/rl_agent/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Othello-Web-app-with-RL/rl_agent/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m     25\u001b[39m eval_results = []\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     V = \u001b[43mtrain_with_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# 評価：固定相手×色交代（例：ヒューリスティック）\u001b[39;00m\n\u001b[32m     29\u001b[39m     er = evaluate_agent(V, games_per_color=\u001b[32m30\u001b[39m, V_opp=LinearValue(), opp_eps=\u001b[32m0.05\u001b[39m, seed=\u001b[32m1000\u001b[39m+s)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_with_report\u001b[39m\u001b[34m(num_games, alpha, eps, seed, trial, report_every)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_with_report\u001b[39m(num_games, alpha, eps, seed, trial=\u001b[38;5;28;01mNone\u001b[39;00m, report_every=\u001b[32m200\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     V, hist = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# 学習中の中間値（黒勝率など）をレポートしてプルーニングに使う\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(num_games, alpha, eps, seed)\u001b[39m\n\u001b[32m      8\u001b[39m history = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_games+\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     w = \u001b[43mplay_one_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     results[w] += \u001b[32m1\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mplay_one_game\u001b[39m\u001b[34m(V, alpha, eps, learn, use_heuristic_opponent, opp_color, opp_eps)\u001b[39m\n\u001b[32m     78\u001b[39m     m = choose_move_opponent(g, p, V, eps=opp_eps)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     m = \u001b[43mchoose_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     82\u001b[39m     g.player = opponent(g.player)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mchoose_move\u001b[39m\u001b[34m(game, player, V, eps)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m moves:\n\u001b[32m     12\u001b[39m     g2 = game.clone(); g2.play(m[\u001b[32m0\u001b[39m], m[\u001b[32m1\u001b[39m], player)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     v = \u001b[43mV\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v > best_v:\n\u001b[32m     15\u001b[39m         best_v = v; best_m = m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mNTupleValue.value\u001b[39m\u001b[34m(self, game, player)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalue\u001b[39m(\u001b[38;5;28mself\u001b[39m, game: Othello, player: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.value_from_indices(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mNTupleValue.indices_for\u001b[39m\u001b[34m(self, game, player)\u001b[39m\n\u001b[32m     49\u001b[39m     s = \u001b[32m0\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, (r,c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(t):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         s += \u001b[38;5;28mself\u001b[39m._cell_trit(b[r][c], player) * \u001b[38;5;28mself\u001b[39m.base_pow[i]\n\u001b[32m     52\u001b[39m     idxs.append(s)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m idxs\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def opponent(c):\n",
    "    \"\"\"現在のプレイヤーの相手を返す\"\"\"\n",
    "    return -c\n",
    "\n",
    "def train_with_report(num_games, alpha, eps, seed, trial=None, report_every=200):\n",
    "    V, hist = train(num_games=num_games, alpha=alpha, eps=eps, seed=seed)\n",
    "    # 学習中の中間値（黒勝率など）をレポートしてプルーニングに使う\n",
    "    if trial is not None:\n",
    "        for h in hist:\n",
    "            if h[\"game\"] % report_every == 0:\n",
    "                trial.report(h[\"black\"], step=h[\"game\"])\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.TrialPruned()\n",
    "    return V\n",
    "\n",
    "def objective(trial):\n",
    "    # --- 探索対象 ---\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-4, 5e-2, log=True)\n",
    "    eps   = trial.suggest_float(\"eps\",   1e-2, 2e-1, log=True)\n",
    "    # 学習ゲーム数は控えめ（例: 2000〜6000）にして、評価にコストを回す\n",
    "    num_games = trial.suggest_int(\"num_games\", 2000, 6000, step=200)\n",
    "\n",
    "    # --- 複数seedでノイズを平均化 ---\n",
    "    seeds = [41, 42, 43]\n",
    "    eval_results = []\n",
    "    for s in seeds:\n",
    "        V = train_with_report(num_games, alpha, eps, s, trial=trial, report_every=200)\n",
    "        # 評価：固定相手×色交代（例：ヒューリスティック）\n",
    "        er = evaluate_agent(V, games_per_color=30, opp_type=\"random\", opp_V=None, opp_eps=0.05, seed=1000+s)\n",
    "        eval_results.append(er[\"winrate\"])\n",
    "    return mean(eval_results)\n",
    "\n",
    "# --- Study 作成：TPE seed固定＋MedianPruner ---\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)  # 再現性のための固定seed（Optuna公式推奨）\n",
    "pruner  = MedianPruner(n_startup_trials=5, n_warmup_steps=5, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"othello_ntuple_eval\",\n",
    "    storage=f\"sqlite:///{optuna_db_path}\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    "    load_if_exists=True,\n",
    "    direction=\"maximize\"\n",
    ")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "bt = study.best_trial\n",
    "params = bt.params\n",
    "print(\"Best params:\", params)  # 例: {'alpha': 0.008, 'eps': 0.15, 'num_games': 4000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d9a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2025\n",
    "V_best, _ = train(num_games=params[\"num_games\"], alpha=params[\"alpha\"], eps=params[\"eps\"], seed=SEED)\n",
    "V_best, _ = train(num_games=2000, alpha=0.1, eps=0.1, seed=SEED)\n",
    "\n",
    "# 4) 任意：保存しておく\n",
    "with open(\"./models/best_ntuple.json\", \"w\") as f:\n",
    "    json.dump(V_best.to_dict(), f)\n",
    "\n",
    "# 5) 人間 vs ベストモデルで対戦（実戦は探索0）\n",
    "human_vs_agent(V_best, human_color=WHITE, seed=SEED)  # 人間が白、エージェントが黒"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
